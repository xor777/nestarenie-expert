import os
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import json

load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if not OPENAI_API_KEY:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    exit(1)

client_openai = OpenAI(api_key=OPENAI_API_KEY)

TEMPERATURE = float(os.getenv('TEMPERATURE', 0.1))
MIN_RELEVANCE = float(os.getenv('MIN_RELEVANCE', 0.9))
MAX_INPUT_TOKENS = int(os.getenv('MAX_INPUT_TOKENS', 1000))
DIRECT_ANSWER_RELEVANCE = float(os.getenv('DIRECT_ANSWER_RELEVANCE', 0.98))
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
if not TELEGRAM_TOKEN:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–æ–∫–µ–Ω TELEGRAM_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    exit(1)

if 'EMBEDDING_MODEL' in os.environ:
    del os.environ['EMBEDDING_MODEL']
load_dotenv()
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')

chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_collection("questions")

def get_embedding(text: str) -> Optional[List[float]]:
    try:
        text = " ".join(text.split())
        
        if len(text) > MAX_INPUT_TOKENS * 4:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –µ–≥–æ —á–∞—Å—Ç—å")
            text = text[:MAX_INPUT_TOKENS * 4]
            
        response = client_openai.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(e)}")
        return None

def save_generated_answer(question: str, answer: str, reference: str = "GPT") -> None:
    try:
        embedding = get_embedding(question)
        if embedding:
            collection.add(
                embeddings=[embedding],
                documents=[question],
                metadatas=[{"answer": answer, "reference": reference, "is_generated": True}],
                ids=[str(collection.count() + 1)]
            )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

def get_relevant_context(query: str, query_embedding: List[float], include_generated: bool = True) -> List[Dict]:
    print(f"searching... [pre-generated {'included' if include_generated else 'excluded'}]")
    
    try:
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5,
            include=["documents", "metadatas", "distances"]
        )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return []
    
    context = []
    for question, metadata, distance in zip(
        results['documents'][0],
        results['metadatas'][0],
        results['distances'][0]
    ):
        relevance = 1 - distance
        is_generated = metadata.get('is_generated', False)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –µ—Å–ª–∏ include_generated=False
        if not include_generated and is_generated:
            print(f"relevance: {relevance} | skipped (generated): {question}")
            continue
            
        if relevance < MIN_RELEVANCE:
            print(f"relevance: {relevance} | skipped (relevance): {question}")
            continue
            
        print(f"relevance: {relevance} | added: {question}")
        context.append({
            "question": question,
            "answer": metadata["answer"],
            "reference": metadata["reference"],
            "relevance": relevance,
            "is_generated": is_generated
        })
    return context

def generate_response(query: str, context: List[Dict]) -> Dict:
    if not context:
        return None
    
    context_text = "\n\n".join([
        f"–§–†–ê–ì–ú–ï–ù–¢ #{i+1}\n–í–û–ü–†–û–°:\n{c['question']}\n–û–¢–í–ï–¢:\n{c['answer']}\nURL:\n{c['reference']}"
        for i, c in enumerate(sorted(context, key=lambda x: x['relevance'], reverse=True))
    ])

    system_message = '''–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞–π—Ç–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–∞—Ç—å –Ω–∞—É—á–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
        1. –°—Ä–∞–≤–Ω–∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö 
        2. –í—ã–±–µ—Ä–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–∏ –ø–æ —Å–º—ã—Å–ª—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—é
        3. –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –¢–û–õ–¨–ö–û –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        4. –í–∫–ª—é—á–∞–π –≤ reference –¢–û–õ–¨–ö–û URL –∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

        –ü–†–ê–í–ò–õ–ê –°–û–°–¢–ê–í–õ–ï–ù–ò–Ø –û–¢–í–ï–¢–ê:
        1. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –ª–æ–≥–∏—á–Ω—ã–º
        2. –í–∫–ª—é—á–∞–π –≤—Å–µ –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏: —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã, –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        3. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
        4. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        5. –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ - —É–∫–∞–∂–∏ –Ω–∞ –Ω–∏—Ö
        6. –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π - –ø—Ä–∏–≤–æ–¥–∏ –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        7. –ò–∑–±–µ–≥–∞–π –æ–±–æ–±—â–µ–Ω–∏–π –±–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–∞–Ω–Ω—ã–º–∏

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
        {
        "answer": "–ü–æ–¥—Ä–æ–±–Ω—ã–π –Ω–∞—É—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤",
        "reference": "URL –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"
        }'''

    user_message = f'''–ö–û–ù–¢–ï–ö–°–¢:
        {context_text}

        –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
        {query}

        –ü–†–û–¶–ï–°–°:
        1. –ù–∞–π–¥–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏
        2. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö
        3. –°–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞—É—á–Ω—ã–π –æ—Ç–≤–µ—Ç
        4. –í–∫–ª—é—á–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ—Ç–∞–ª–∏
        5. –î–æ–±–∞–≤—å –≤ reference –¢–û–õ–¨–ö–û URL –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        6. –ü—Ä–æ–≤–µ—Ä—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É –æ—Ç–≤–µ—Ç–∞

        –í–µ—Ä–Ω–∏ JSON —Å –ø–æ–ª—è–º–∏ "answer" –∏ "reference"'''
    
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=TEMPERATURE,
            max_tokens=1000,
            response_format={"type": "json_object"}
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
        return None

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –Ω–∞—É–∫–∏ –æ —Å—Ç–∞—Ä–µ–Ω–∏–∏. "
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–∏—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = update.message.text
    user = update.effective_user
    
    print(f"\n{'='*20} –≤–æ–ø—Ä–æ—Å: {'='*20}")
    print(f"Username: {user.username or user.id} | Name: {user.first_name}")
    print(f"Question: {query}")
    print(f"{'='*60}")
    
    await update.message.chat.send_action(action="typing")
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–∏–Ω —Ä–∞–∑
    query_embedding = get_embedding(query)
    if not query_embedding:
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞.")
        return
    
    # –ò—â–µ–º —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤
    relevant_context = get_relevant_context(query, query_embedding=query_embedding, include_generated=True)
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –∑–Ω–∞—á–∏—Ç –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ
    if not relevant_context:
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
        return
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    if relevant_context[0]['relevance'] >= DIRECT_ANSWER_RELEVANCE:
        most_relevant = relevant_context[0]
        emoji = "üöÄ" if most_relevant['is_generated'] else "üìñ"
        references = "\n".join(most_relevant['reference'].split())
        response = f"{emoji} {most_relevant['answer']}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n{references}"
        
    # –ï—Å–ª–∏ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    else:
        original_context = get_relevant_context(query, query_embedding=query_embedding, include_generated=False)
        if not original_context:
            await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
            return
            
        generated = generate_response(query, original_context)
        if not generated:
            await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.")
            return
            
        response_data = json.loads(generated)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_generated_answer(query, response_data["answer"], response_data["reference"])
        references = "\n".join(response_data["reference"].split())
        response = f"üß† {response_data['answer']}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n{references}"
    
    if len(response) > 4096:
        for i in range(0, len(response), 4096):
            await update.message.reply_text(response[i:i+4096])
    else:
        await update.message.reply_text(response)

def main() -> None:
    if not os.path.exists("./chroma_db"):
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ load_dataset.py")
        return

    application = Application.builder().token(TELEGRAM_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    application.run_polling()

if __name__ == "__main__":
    main() 