import os
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import json

load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if not OPENAI_API_KEY:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    exit(1)

client_openai = OpenAI(api_key=OPENAI_API_KEY)

TEMPERATURE = float(os.getenv('TEMPERATURE', 0.1))
MIN_RELEVANCE = float(os.getenv('MIN_RELEVANCE', 0.9))
MAX_INPUT_TOKENS = int(os.getenv('MAX_INPUT_TOKENS', 1000))
DIRECT_ANSWER_RELEVANCE = float(os.getenv('DIRECT_ANSWER_RELEVANCE', 0.98))
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')
if not TELEGRAM_TOKEN:
    print("–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–æ–∫–µ–Ω TELEGRAM_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    exit(1)

if 'EMBEDDING_MODEL' in os.environ:
    del os.environ['EMBEDDING_MODEL']
load_dotenv()
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')

chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_collection("questions")

def get_embedding(text: str) -> Optional[List[float]]:
    try:
        text = " ".join(text.split())
        
        if len(text) > MAX_INPUT_TOKENS * 4:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –µ–≥–æ —á–∞—Å—Ç—å")
            text = text[:MAX_INPUT_TOKENS * 4]
            
        response = client_openai.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(e)}")
        return None

def save_generated_answer(question: str, answer: str, reference: str = "GPT") -> None:
    try:
        embedding = get_embedding(question)
        if embedding:
            collection.add(
                embeddings=[embedding],
                documents=[question],
                metadatas=[{"answer": answer, "reference": reference, "is_generated": True}],
                ids=[str(collection.count() + 1)]
            )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

def get_relevant_context(query: str) -> List[Dict]:
    query_embedding = get_embedding(query)
    if not query_embedding:
        return []
        
    try:
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5,
            include=["documents", "metadatas", "distances"]
        )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return []
    
    context = []
    for question, metadata, distance in zip(
        results['documents'][0],
        results['metadatas'][0],
        results['distances'][0]
    ):
        relevance = 1 - distance
        if relevance < MIN_RELEVANCE:
            print(f"relevance: {relevance} | skipped: {question}")
            continue
        print(f"relevance: {relevance} | added: {question}")
        context.append({
            "question": question,
            "answer": metadata["answer"],
            "reference": metadata["reference"],
            "relevance": relevance,
            "is_generated": metadata.get("is_generated", False)
        })
    return context

def generate_response(query: str, context: List[Dict]) -> Dict:
    if not context:
        return None
    
    context_text = "\n\n".join([
        f"–§–†–ê–ì–ú–ï–ù–¢ #{i+1}\n–ö–û–ù–¢–ï–ö–°–¢:\n{c['answer']}\nURL:\n{c['reference']}"
        for i, c in enumerate(sorted(context, key=lambda x: x['relevance'], reverse=True))
    ])

    system_message = '''–¢—ã - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –∏ –Ω–∞—É–∫–µ –æ —Å—Ç–∞—Ä–µ–Ω–∏–∏.

        –¢–í–û–Ø –ó–ê–î–ê–ß–ê:
        –î–∞—Ç—å —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –°–¢–†–û–ì–û –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
        –¢—ã –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å JSON –æ–±—ä–µ–∫—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞:
        {
            "answer": "–¢–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –≤–∫–ª—é—á–∞—é—â–∏–π –í–°–ï –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
            "reference": "–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö URL, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π"
        }

        –ü–†–ê–í–ò–õ–ê –°–û–°–¢–ê–í–õ–ï–ù–ò–Ø –û–¢–í–ï–¢–ê:
        1. –í –ø–æ–ª–µ "answer" –≤–∫–ª—é—á–∞–π –í–°–Æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        2. –ò—Å–ø–æ–ª—å–∑—É–π –≤—Å–µ –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, —Ü–∏—Ñ—Ä—ã, —Ñ–∞–∫—Ç—ã –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è
        3. –ù–µ –ø—Ä–æ–ø—É—Å–∫–∞–π –∑–Ω–∞—á–∏–º—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
        4. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
        5. –í –ø–æ–ª–µ "reference" —É–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ –£–ù–ò–ö–ê–õ–¨–ù–´–ï URL —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π'''

    user_message = f'''–ö–û–ù–¢–ï–ö–°–¢:
        {context_text}

        –í–û–ü–†–û–°:
        {query}

        –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
        1. –í–µ—Ä–Ω–∏ JSON –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏ "answer" –∏ "reference"
        2. –í –ø–æ–ª–µ "answer" –¥–∞–π –ü–û–î–†–û–ë–ù–´–ô –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        3. –í –ø–æ–ª–µ "reference" —É–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ URL —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π
        4. URL –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã —Ç–æ—á–Ω–æ, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π'''
    
    try:
        response = client_openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=TEMPERATURE,
            max_tokens=1000,
            response_format={"type": "json_object"}
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
        return None

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –Ω–∞—É–∫–∏ –æ —Å—Ç–∞—Ä–µ–Ω–∏–∏. "
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–∏—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = update.message.text
    user = update.effective_user
    
    print(f"\n{'='*20} –≤–æ–ø—Ä–æ—Å: {'='*20}")
    print(f"Username: {user.username or user.id} | Name: {user.first_name}")
    print(f"Question: {query}")
    print(f"{'='*60}")
    
    await update.message.chat.send_action(action="typing")
    
    relevant_context = get_relevant_context(query)
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –∑–Ω–∞—á–∏—Ç –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ
    if not relevant_context:
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
        return
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    if relevant_context[0]['relevance'] >= DIRECT_ANSWER_RELEVANCE:
        most_relevant = relevant_context[0]
        emoji = "üöÄ" if most_relevant['is_generated'] else "üìñ"
        response = f"{emoji} {most_relevant['answer']}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {most_relevant['reference']}"
        
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
    else:
        generated = generate_response(query, relevant_context)
        if not generated:
            await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.")
            return
            
        response_data = json.loads(generated)
        save_generated_answer(query, response_data["answer"], response_data["reference"])
        response = f"üß† {response_data['answer']}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {response_data['reference']}"
    
    if len(response) > 4096:
        for i in range(0, len(response), 4096):
            await update.message.reply_text(response[i:i+4096])
    else:
        await update.message.reply_text(response)

def main() -> None:
    if not os.path.exists("./chroma_db"):
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ load_dataset.py")
        return

    application = Application.builder().token(TELEGRAM_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    application.run_polling()

if __name__ == "__main__":
    main() 