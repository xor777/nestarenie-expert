import os
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
from typing import Optional, List, Dict, TypedDict
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import json
from dataclasses import dataclass

load_dotenv()

@dataclass
class Config:
    openai_api_key: str
    telegram_token: str
    temperature: float
    min_relevance: float
    max_input_tokens: int
    direct_answer_relevance: float
    embedding_model: str
    generation_model: str
    
    @classmethod
    def from_env(cls) -> 'Config':
        load_dotenv()
        
        openai_api_key = os.getenv('OPENAI_API_KEY')
        if not openai_api_key:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            
        telegram_token = os.getenv('TELEGRAM_TOKEN')
        if not telegram_token:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω —Ç–æ–∫–µ–Ω TELEGRAM_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
            
        return cls(
            openai_api_key=openai_api_key,
            telegram_token=telegram_token,
            temperature=float(os.getenv('TEMPERATURE', '0.1')),
            min_relevance=float(os.getenv('MIN_RELEVANCE', '0.9')),
            max_input_tokens=int(os.getenv('MAX_INPUT_TOKENS', '1000')),
            direct_answer_relevance=float(os.getenv('DIRECT_ANSWER_RELEVANCE', '0.98')),
            embedding_model=os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small'),
            generation_model=os.getenv('GENERATION_MODEL', 'gpt-4')
        )

config = Config.from_env()

client_openai = OpenAI(api_key=config.openai_api_key)
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_collection("questions")

class ContextItem(TypedDict):
    question: str
    answer: str
    reference: str
    relevance: float
    is_generated: bool

class GeneratedResponse(TypedDict):
    answer: str
    reference: str

def get_embedding(text: str) -> Optional[List[float]]:
    try:
        text = " ".join(text.split())
        
        if len(text) > config.max_input_tokens * 4:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Ç–æ–ª—å–∫–æ –µ–≥–æ —á–∞—Å—Ç—å")
            text = text[:config.max_input_tokens * 4]
            
        response = client_openai.embeddings.create(
            model=config.embedding_model,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(e)}")
        return None

def save_generated_answer(question: str, answer: str, reference: str, embedding: Optional[List[float]] = None) -> None:
    try:
        if not embedding:
            embedding = get_embedding(question)
        if embedding:
            collection.add(
                embeddings=[embedding],
                documents=[question],
                metadatas=[{"answer": answer, "reference": reference, "is_generated": True}],
                ids=[str(collection.count() + 1)]
            )
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")

def get_relevant_context(query: str, query_embedding: List[float], include_generated: bool = True) -> List[ContextItem]:
    print(f"searching... [pre-generated {'included' if include_generated else 'excluded'}]")
    
    try:
        query_params = {
            "query_embeddings": [query_embedding],
            "n_results": 5,
            "include": ["documents", "metadatas", "distances"]
        }
        if not include_generated:
            query_params["where"] = {"is_generated": False}
            
        results = collection.query(**query_params)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        return []
    
    context = []
    for question, metadata, distance in zip(
        results['documents'][0],
        results['metadatas'][0],
        results['distances'][0]
    ):
        relevance = 1 - distance
        if relevance < config.min_relevance:
            print(f"relevance: {relevance} | skipped (relevance): {question}")
            continue
            
        print(f"relevance: {relevance} | added: {question}")
        context.append({
            "question": question,
            "answer": metadata["answer"],
            "reference": metadata["reference"],
            "relevance": relevance,
            "is_generated": metadata.get('is_generated', False)
        })
    return context

def generate_response(query: str, context: List[ContextItem]) -> Optional[str]:
    if not context:
        return None
    
    context_text = "\n\n".join([
        f"–§–†–ê–ì–ú–ï–ù–¢ #{i+1}\n–í–û–ü–†–û–°:\n{c['question']}\n–û–¢–í–ï–¢:\n{c['answer']}\nURL:\n{c['reference']}"
        for i, c in enumerate(sorted(context, key=lambda x: x['relevance'], reverse=True))
    ])

    system_message = '''–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞—É—á–Ω–æ —Ç–æ—á–Ω—ã–µ, —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
        1. –°—Ä–∞–≤–Ω–∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö
        2. –í—ã–±–µ—Ä–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±–ª–∏–∑–∫–∏ –ø–æ —Å–º—ã—Å–ª—É
        3. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        4. –í reference –≤–∫–ª—é—á–∞–π –¢–û–õ–¨–ö–û URL —Ç–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —á—å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–æ—à–ª–∞ –≤ –æ—Ç–≤–µ—Ç
        5. –ù–ò–ö–û–ì–î–ê –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º

        –ü–†–ê–í–ò–õ–ê –°–û–°–¢–ê–í–õ–ï–ù–ò–Ø –û–¢–í–ï–¢–ê:
        1. –ü–æ–ª–Ω–æ—Ç–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
        - –ù–∞—á–Ω–∏ —Å –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        - –†–∞—Å–∫—Ä–æ–π –≤—Å–µ –≤–∞–∂–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã
        - –í–∫–ª—é—á–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, —Ü–∏—Ñ—Ä—ã, —Ç–µ—Ä–º–∏–Ω—ã
        - –î–æ–±–∞–≤—å –≤–∞–∂–Ω—ã–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è –∏ –Ω—é–∞–Ω—Å—ã
        - –°–æ—Ö—Ä–∞–Ω—è–π –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å

        2. –ù–∞—É—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å:
        - –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        - –ü—Ä–∏–≤–æ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        - –£–∫–∞–∑—ã–≤–∞–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        - –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π - –æ—Ç—Ä–∞–∑–∏ —Ä–∞–∑–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏

        3. –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å:
        - –ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –Ω–µ—Å—Ç–∏ –∑–Ω–∞—á–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        - –ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ –∏ —Å–∞–º–æ–æ—á–µ–≤–∏–¥–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        - –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π—Å—è, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –Ω—É–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
        - –†–∞—Å–∫—Ä—ã–≤–∞–π —Å—É—Ç—å –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
        {
        "answer": "–ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π",
        "reference": "URL —Ç–æ–ª—å–∫–æ —Ç–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –≤ –æ—Ç–≤–µ—Ç–µ"
        }'''

    user_message = f'''–ö–û–ù–¢–ï–ö–°–¢:
        {context_text}

        –í–û–ü–†–û–°:
        {query}

        –ü–†–û–¶–ï–°–°:
        1. –ù–∞–π–¥–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏
        2. –ü—Ä–æ–≤–µ—Ä—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        3. –°–æ—Å—Ç–∞–≤—å –ø–æ–ª–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:
        - –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ
        - –í–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –∏ –Ω—é–∞–Ω—Å—ã
        - –ù–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∏—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        4. –£–±–µ–¥–∏—Å—å, —á—Ç–æ –∫–∞–∂–¥—ã–π –≤–∫–ª—é—á–µ–Ω–Ω—ã–π URL —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        5. –ü—Ä–æ–≤–µ—Ä—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        –í–µ—Ä–Ω–∏ JSON —Å –ø–æ–ª—è–º–∏ "answer" –∏ "reference"'''
    
    try:
        response = client_openai.chat.completions.create(
            model=config.generation_model,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=config.temperature,
            max_tokens=1000,
            response_format={"type": "json_object"}
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
        return None

def format_references(reference: str) -> str:
    if not reference or reference.isspace():
        return ""
    return f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n{'\n'.join(reference.split())}"

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –Ω–∞—É–∫–∏ –æ —Å—Ç–∞—Ä–µ–Ω–∏–∏. "
        "–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–∏—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
    )

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = update.message.text
    user = update.effective_user
    
    print(f"\n{'='*20} –≤–æ–ø—Ä–æ—Å: {'='*20}")
    print(f"Username: {user.username or user.id} | Name: {user.first_name}")
    print(f"Question: {query}")
    print(f"{'='*60}")
    
    await update.message.chat.send_action(action="typing")
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–∏–Ω —Ä–∞–∑
    query_embedding = get_embedding(query)
    if not query_embedding:
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞.")
        return
    
    # –ò—â–µ–º —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤
    relevant_context = get_relevant_context(query, query_embedding=query_embedding, include_generated=True)
    
    # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π, –∑–Ω–∞—á–∏—Ç –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ
    if not relevant_context:
        await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
        return
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    if relevant_context[0]['relevance'] >= config.direct_answer_relevance:
        most_relevant = relevant_context[0]
        emoji = "üöÄ" if most_relevant['is_generated'] else "üìñ"
        response = f"{emoji} {most_relevant['answer']}{format_references(most_relevant['reference'])}"
        
    # –ï—Å–ª–∏ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ —Å –≤—ã—Å–æ–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
    else:
        original_context = get_relevant_context(query, query_embedding=query_embedding, include_generated=False)
        if not original_context:
            await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É.")
            return
            
        generated = generate_response(query, original_context)
        if not generated:
            await update.message.reply_text("–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.")
            return
            
        response_data = json.loads(generated)
        save_generated_answer(
            question=query, 
            answer=response_data["answer"], 
            reference=response_data["reference"],
            embedding=query_embedding
        )
        response = f"üß† {response_data['answer']}{format_references(response_data['reference'])}"
    
    await update.message.reply_text(response)

def main() -> None:
    if not os.path.exists("./chroma_db"):
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ load_dataset.py")
        return

    application = Application.builder().token(config.telegram_token).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    application.run_polling()

if __name__ == "__main__":
    main() 